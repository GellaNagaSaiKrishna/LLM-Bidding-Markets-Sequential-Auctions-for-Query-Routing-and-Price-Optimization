{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb1ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31bde5cc-79c4-422e-9ca3-56cfccaafe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries of the dataset:\n",
      "   Sno Capital City               Country Continent Latitude Longitude\n",
      "0    1    Abu Dhabi  United Arab Emirates      Asia   24.28N    54.22E\n",
      "1    2        Abuja               Nigeria    Africa   09.05N    07.32E\n",
      "2    3        Accra                 Ghana    Africa   05.35N    00.06W\n",
      "3    4  Addis Ababa              Ethiopia    Africa   09.02N    38.42E\n",
      "4    5      Algiers               Algeria    Africa   36.42N    03.08E\n",
      "\n",
      "--- Column Information ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196 entries, 0 to 195\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Sno           196 non-null    int64 \n",
      " 1   Capital City  196 non-null    object\n",
      " 2   Country       196 non-null    object\n",
      " 3   Continent     196 non-null    object\n",
      " 4   Latitude      196 non-null    object\n",
      " 5   Longitude     196 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 9.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the full, absolute path where your kagglehub download is located\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "\n",
    "try:\n",
    "    # 1. Load the data into a DataFrame\n",
    "    # Note the use of the full path, which should work in your local VS Code environment.\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Print the first 5 rows (and all columns)\n",
    "    print(\"First 5 entries of the dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 3. Optional: Print the column names and data types\n",
    "    print(\"\\n--- Column Information ---\")\n",
    "    print(df.info())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at the specified path: {file_path}\")\n",
    "    print(\"Please double-check the path or ensure the file was successfully downloaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8121df-58ab-4010-8443-3835710a4f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/home/gella.saikrishna/my_llama_env/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/home/gella.saikrishna/my_llama_env/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/transformers/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     30\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     is_pretty_midi_available,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/transformers/utils/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/huggingface_hub/__init__.py:1036\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[0;32m-> 1036\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_commit_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     57\u001b[0m     CommitOperation,\n\u001b[1;32m     58\u001b[0m     CommitOperationAdd,\n\u001b[1;32m     59\u001b[0m     CommitOperationCopy,\n\u001b[1;32m     60\u001b[0m     CommitOperationDelete,\n\u001b[1;32m     61\u001b[0m     _fetch_files_to_copy,\n\u001b[1;32m     62\u001b[0m     _fetch_upload_modes,\n\u001b[1;32m     63\u001b[0m     _prepare_commit_payload,\n\u001b[1;32m     64\u001b[0m     _upload_files,\n\u001b[1;32m     65\u001b[0m     _warn_on_overwriting_operations,\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inference_endpoints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceEndpoint, InferenceEndpointType\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_jobs_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JobInfo, JobSpec, ScheduledJobInfo, _create_job_spec\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError, HfHubHTTPError, XetAuthorizationError, XetRefreshTokenError\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_url\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlfs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UploadInfo, lfs_upload, post_lfs_batch_info\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     FORBIDDEN_FOLDERS,\n\u001b[1;32m     24\u001b[0m     XetTokenType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     validate_hf_hub_args,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/my_llama_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:36\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     HUGGINGFACE_CO_URL_TEMPLATE,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     HUGGINGFACE_HUB_CACHE,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     EntryNotFoundError,\n\u001b[1;32m     29\u001b[0m     FileMetadataError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     RevisionNotFoundError,\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     OfflineModeIsEnabled,\n\u001b[1;32m     38\u001b[0m     SoftTemporaryDirectory,\n\u001b[1;32m     39\u001b[0m     WeakFileLock,\n\u001b[1;32m     40\u001b[0m     XetFileData,\n\u001b[1;32m     41\u001b[0m     build_hf_headers,\n\u001b[1;32m     42\u001b[0m     get_fastai_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     get_fastcore_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     get_graphviz_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     get_jinja_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     get_pydot_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     get_tf_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     get_torch_version,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     hf_raise_for_status,\n\u001b[1;32m     50\u001b[0m     is_fastai_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     is_fastcore_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     is_graphviz_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     is_jinja_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     is_pydot_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     is_tf_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     is_torch_available,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     logging,\n\u001b[1;32m     58\u001b[0m     parse_xet_file_data_from_response,\n\u001b[1;32m     59\u001b[0m     refresh_xet_connection_info,\n\u001b[1;32m     60\u001b[0m     reset_sessions,\n\u001b[1;32m     61\u001b[0m     tqdm,\n\u001b[1;32m     62\u001b[0m     validate_hf_hub_args,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_http\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _adjust_range_header, http_backoff\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_runtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PY_VERSION, is_xet_available  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'reset_sessions' from 'huggingface_hub.utils' (/home/gella.saikrishna/my_llama_env/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "\n",
    "# Suppress warnings from the transformers library for a clean output\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# IMPORTANT: Use the full, absolute path from your environment\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "QUERY_COLUMN = 'Country' \n",
    "EVAL_COLUMNS = ['Capital City', 'Continent', 'Latitude', 'Longitude'] \n",
    "ROWS_TO_EVALUATE = 5 # As requested, we will evaluate only the first 5 entries\n",
    "\n",
    "# --- 1. LLM Initialization (DeepSeek-R1-Distill-Qwen-1.5B) ---\n",
    "try:\n",
    "    print(\" Initializing DeepSeek-R1-Distill-Qwen-1.5B pipeline...\")\n",
    "    \n",
    "    # NOTE: You may need to run huggingface_hub.login() separately if this is your first time.\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        # Using torch.float16 and 'auto' device map for typical GPU setups\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    pipe_tokenizer = pipe.tokenizer\n",
    "    print(\" DeepSeek pipeline loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL: Failed to load DeepSeek pipeline. Check environment, token, and hardware.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    # Setting pipe to None so the rest of the script handles the failure gracefully\n",
    "    pipe = None \n",
    "    pipe_tokenizer = None\n",
    "\n",
    "\n",
    "# --- 2. LLM Prediction Function ---\n",
    "def get_llm_prediction(country_name, pipe, pipe_tokenizer):\n",
    "    \"\"\"\n",
    "    Takes a country name, queries the LLM pipeline, and returns a dictionary \n",
    "    with the expected columns, or empty strings on failure.\n",
    "    \"\"\"\n",
    "    if pipe is None:\n",
    "        return {col: \"LLM_ERROR\" for col in EVAL_COLUMNS}\n",
    "        \n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Define terminators for Qwen-based models\n",
    "    terminators = [pipe_tokenizer.eos_token_id]\n",
    "    qwen_eot_id = pipe_tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    if qwen_eot_id is not None:\n",
    "        terminators.append(qwen_eot_id)\n",
    "    \n",
    "    try:\n",
    "        outputs = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "        \n",
    "        # Use regex to robustly find the JSON object in the output\n",
    "        json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            json_string = json_match.group(0)\n",
    "            return json.loads(json_string)\n",
    "        else:\n",
    "            print(f\"Warning: No JSON found for {country_name}.\")\n",
    "            return {col: \"\" for col in EVAL_COLUMNS}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM inference for {country_name}: {e}\")\n",
    "        return {col: \"\" for col in EVAL_COLUMNS}\n",
    "\n",
    "\n",
    "# --- 3. Evaluation Logic ---\n",
    "def calculate_accuracy(df, query_col, eval_cols, pipe, pipe_tokenizer):\n",
    "    \"\"\"Loops through the dataframe rows, gets LLM predictions, and calculates accuracy.\"\"\"\n",
    "    \n",
    "    # Normalize ground truth data for comparison\n",
    "    df_eval = df.head(ROWS_TO_EVALUATE).copy()\n",
    "    for col in eval_cols:\n",
    "        df_eval[col] = df_eval[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    correct_counts = {col: 0 for col in eval_cols}\n",
    "    total_count = len(df_eval)\n",
    "\n",
    "    print(f\"\\n--- Starting Evaluation for First {total_count} Entries ---\")\n",
    "    \n",
    "    for index, row in df_eval.iterrows():\n",
    "        country = row[query_col]\n",
    "        llm_response_dict = get_llm_prediction(country, pipe, pipe_tokenizer)\n",
    "        \n",
    "        print(f\"\\n[{index+1}/{total_count}] Country: {country}\")\n",
    "        \n",
    "        for col in eval_cols:\n",
    "            true_value = row[col]\n",
    "            predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "            \n",
    "            is_correct = (predicted_value == true_value)\n",
    "            \n",
    "            # Special Robust Comparison for Latitude/Longitude (within a small tolerance)\n",
    "            if col in ['Latitude', 'Longitude']:\n",
    "                # Clean strings to extract numbers only\n",
    "                true_num_str = re.sub(r'[^\\d\\.\\-]', '', true_value.replace('n', '').replace('s', '').replace('e', '').replace('w', ''))\n",
    "                pred_num_str = re.sub(r'[^\\d\\.\\-]', '', predicted_value.replace('n', '').replace('s', '').replace('e', '').replace('w', ''))\n",
    "                \n",
    "                try:\n",
    "                    true_num = float(true_num_str)\n",
    "                    pred_num = float(pred_num_str)\n",
    "                    \n",
    "                    # Check if the absolute difference is less than 0.1 (a small tolerance)\n",
    "                    if abs(true_num - pred_num) < 0.1:\n",
    "                        is_correct = True\n",
    "                    else:\n",
    "                        is_correct = False\n",
    "                except ValueError:\n",
    "                    # If conversion to float fails, it's considered incorrect\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_correct:\n",
    "                correct_counts[col] += 1\n",
    "                result_status = \"CORRECT\"\n",
    "            else:\n",
    "                result_status = \"INCORRECT\"\n",
    "                \n",
    "            print(f\"  - {col}: {result_status} (True: {true_value}, Predicted: {predicted_value})\")\n",
    "            \n",
    "    # Calculate final accuracy percentages\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}% ({correct_counts[col]}/{total_count})\" \n",
    "        for col in eval_cols\n",
    "    }\n",
    "    \n",
    "    return efficiency, total_count, correct_counts\n",
    "\n",
    "\n",
    "# --- 4. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    if pipe is None:\n",
    "        print(\"\\nCannot proceed with evaluation due to LLM initialization failure.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Display the first 5 entries (matching your provided image)\n",
    "        print(\"\\nFirst 5 entries of the dataset (Ground Truth):\")\n",
    "        # Ensure Sno is displayed as an integer for readability\n",
    "        data_display = data.head(ROWS_TO_EVALUATE).copy()\n",
    "        data_display['Sno'] = data_display['Sno'].astype(int)\n",
    "        print(data_display.to_string(index=False))\n",
    "        \n",
    "        # Run the evaluation\n",
    "        efficiency_results, total, correct = calculate_accuracy(data, QUERY_COLUMN, EVAL_COLUMNS, pipe, pipe_tokenizer)\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ðŸ§  DeepSeek Evaluation Results (Accuracy for First 5 Entries)\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total Countries Evaluated: {total}\")\n",
    "        \n",
    "        results_table = pd.DataFrame([efficiency_results]).T\n",
    "        results_table.columns = ['Accuracy']\n",
    "        results_table.index.name = 'Column'\n",
    "        \n",
    "        print(\"\\nAccuracy by Column:\")\n",
    "        # Display the results in a formatted table\n",
    "        print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d42d40e-f805-4cb8-b5dd-8c13c4b2e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m791.7/791.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 1.0.1\n",
      "    Uninstalling huggingface-hub-1.0.1:\n",
      "      Successfully uninstalled huggingface-hub-1.0.1\n",
      "\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/home/gella.saikrishna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/home/gella.saikrishna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbac75a-097b-4eb2-9751-52a9d8fe3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/gella.saikrishna/.local/lib/python3.10/site-packages (2.9.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (493 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m493.4/493.4 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=0.8.5 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: filelock in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: jinja2 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/gella.saikrishna/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.9.0+cpu torchvision-0.24.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu \n",
    "# OR the specific command from the PyTorch site for your CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4146285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Constants ---\n",
    "C_UCB = 0.5\n",
    "MODEL_CONFIG = {\n",
    "    # Model-1: High cost/High reward prob (Simulated 3B)\n",
    "    'Model-1': {'cost_per_token': 0.6, 'capacity': 3e9},\n",
    "    # Model-2: Low cost/Low reward prob (Simulated 1B)\n",
    "    'Model-2': {'cost_per_token': 0.1, 'capacity': 1e9}\n",
    "}\n",
    "REWARD_STRUCTURE = {\n",
    "    # Assuming the total max reward of 70 in the final simulation output\n",
    "    # corresponds to a scaled-up version of the original structure.\n",
    "    # We will use the original structure for the logic and scale up the final result for presentation.\n",
    "    'Capital City': 10, 'Continent': 10, 'Latitude': 25, 'Longitude': 25\n",
    "}\n",
    "MAX_REWARD = sum(REWARD_STRUCTURE.values()) # 70 points, matching the output\n",
    "DF_FILE_NAME = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "SIMULATION_ROUNDS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0f06e0-64ee-4076-b6e6-83847fedda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM Simulation Functions (using synthetic logic) ---\n",
    "\n",
    "def simulate_llm_tokens_and_performance(country: str, model_name: str) -> Tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Simulates token usage, cost calculation, and reward for a single country query.\n",
    "    \n",
    "    Returns: (total_reward, total_cost, total_tokens)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Simulate Token Usage\n",
    "    input_tokens = 50 \n",
    "    country_length_factor = len(country) // 3\n",
    "    output_tokens_base = 40 + country_length_factor\n",
    "    output_tokens_variation = 5\n",
    "\n",
    "    # Determine verbosity based on model size (FIXED: Numerical comparison)\n",
    "    if MODEL_CONFIG[model_name]['capacity'] > 1e9: \n",
    "        # Larger model (Model-1: 3e9) might be slightly more verbose\n",
    "        output_tokens = output_tokens_base + random.randint(0, output_tokens_variation)\n",
    "    else:\n",
    "        # Smaller model (Model-2: 1e9) might be slightly less verbose/more concise\n",
    "        output_tokens = output_tokens_base + random.randint(-output_tokens_variation, 0)\n",
    "\n",
    "    output_tokens = max(1, output_tokens)\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "\n",
    "    # 2. Calculate Total Cost\n",
    "    cost_per_token = MODEL_CONFIG[model_name]['cost_per_token']\n",
    "    total_cost = total_tokens * cost_per_token\n",
    "\n",
    "    # 3. Simulate Performance/Reward\n",
    "    # Model-1 is better (90% base correctness), Model-2 is worse (70% base correctness)\n",
    "    correctness_base = 0.9 if model_name == 'Model-1' else 0.7 \n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _, reward_points in REWARD_STRUCTURE.items():\n",
    "        # Introduce randomness around the base correctness probability\n",
    "        correctness_prob = correctness_base + random.uniform(-0.1, 0.05)\n",
    "        # Binary outcome: LLM is either correct (1) or incorrect (0)\n",
    "        is_correct = 1 if random.random() < correctness_prob else 0\n",
    "        total_reward += is_correct * reward_points\n",
    "        \n",
    "    return total_reward, total_cost, total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6030a4f-fa7c-4e36-8b56-d8cc3948fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- UCB and Reverse Myerson Implementation ---\n",
    "\n",
    "class LLMBanditSelector:\n",
    "    \"\"\"Implements UCB and Reverse Myerson's Virtual Valuation.\"\"\"\n",
    "    def __init__(self, models: List[str], c_ucb: float):\n",
    "        self.models = models\n",
    "        self.K = len(models)\n",
    "        self.c_ucb = c_ucb\n",
    "        self.t = 0\n",
    "        \n",
    "        # UCB State\n",
    "        self.N_a = {model: 0 for model in models}\n",
    "        self.Q_a = {model: 0.0 for model in models}\n",
    "        \n",
    "        # Reverse Myerson State\n",
    "        self.a_history = {model: [] for model in models}  \n",
    "        \n",
    "    def _compute_ucb_index(self, model: str) -> float:\n",
    "        \"\"\"Calculates the UCB index for a given model (Q_t(a) + exploration_term).\"\"\"\n",
    "        if self.N_a[model] == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # UCB Exploration Term: c * sqrt(ln(t) / N_a)\n",
    "        exploration_term = self.c_ucb * math.sqrt(math.log(self.t) / self.N_a[model])\n",
    "        ucb_index = self.Q_a[model] + exploration_term\n",
    "        return ucb_index\n",
    "    \n",
    "    def _get_empirical_pdf_cdf(self, a_values: List[float], current_a: float, bin_count: int = 20) -> Tuple[float, float]:\n",
    "        \"\"\"Calculates empirical PDF and CDF values for the current 'a' based on history.\"\"\"\n",
    "        if not a_values:\n",
    "            return 1.0, 0.5 \n",
    "\n",
    "        a_all = np.array(a_values + [current_a])\n",
    "        min_a, max_a = a_all.min(), a_all.max()\n",
    "        \n",
    "        if min_a == max_a:\n",
    "            return 1.0, 1.0 if current_a >= min_a else 0.0\n",
    "            \n",
    "        bins = np.linspace(min_a, max_a, bin_count + 1)\n",
    "        \n",
    "        counts, bin_edges = np.histogram(a_values, bins=bins, density=False)\n",
    "        total_samples = len(a_values)\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "        \n",
    "        current_bin_index = np.digitize(current_a, bin_edges) - 1\n",
    "        current_bin_index = np.clip(current_bin_index, 0, bin_count - 1)\n",
    "        \n",
    "        # Empirical PDF (density approximation)\n",
    "        pdf_val = (counts[current_bin_index] / total_samples) / bin_width\n",
    "        \n",
    "        # Empirical CDF\n",
    "        cdf_val = np.sum(counts[:current_bin_index + 1]) / total_samples\n",
    "        \n",
    "        pdf_val = max(pdf_val, 1e-6) # Prevent division by zero\n",
    "        \n",
    "        return pdf_val, cdf_val\n",
    "\n",
    "    def _compute_virtual_valuation(self, a: float, model: str) -> float:\n",
    "        \"\"\"Reverse Myerson Virtual Valuation: a + (CDF(a) / PDF(a))\"\"\"\n",
    "        a_history = self.a_history[model]\n",
    "        pdf_a, cdf_a = self._get_empirical_pdf_cdf(a_history, a)\n",
    "        \n",
    "        # This formula is used for the valuation based on the bid 'a'\n",
    "        virtual_valuation = a + (cdf_a / pdf_a)\n",
    "        return virtual_valuation\n",
    "\n",
    "    def select_model(self, country: str) -> str:\n",
    "        \"\"\"Selects the optimal model.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initial exploration: Ensure every arm is pulled once\n",
    "        for model in self.models:\n",
    "            if self.N_a[model] == 0:\n",
    "                return model\n",
    "\n",
    "        virtual_valuations: Dict[str, float] = {}\n",
    "        \n",
    "        for model in self.models:\n",
    "            # 1. Simulate the hypothetical outcome (Reward, Cost)\n",
    "            # NOTE: simulate_llm_tokens_and_performance must be defined globally\n",
    "            # reward, cost, _ = simulate_llm_tokens_and_performance(country, model) \n",
    "            reward, cost, _ = 0, 0, 0 # Placeholder for missing function\n",
    "            \n",
    "            # 2. Calculate UCB Index and Exploration Term\n",
    "            ucb_index = self._compute_ucb_index(model)\n",
    "            exploration_term = ucb_index - self.Q_a[model] \n",
    "            \n",
    "            # 3. Calculate 'a' value: a = Reward + UCB_Exploration_Term - Cost\n",
    "            a = reward + exploration_term - cost\n",
    "            \n",
    "            # 4. Compute Reverse Myerson Virtual Valuation\n",
    "            virtual_valuation = self._compute_virtual_valuation(a, model)\n",
    "            virtual_valuations[model] = virtual_valuation\n",
    "            \n",
    "        # 5. Select the model with the LOWEST Virtual Valuation\n",
    "        selected_model = min(virtual_valuations, key=virtual_valuations.get)\n",
    "        \n",
    "        return selected_model\n",
    "\n",
    "    def update_model_stats(self, selected_model: str, reward: float, cost: float):\n",
    "        \"\"\"Updates the UCB and Reverse Myerson's history for the selected model.\"\"\"\n",
    "        \n",
    "        # 1. UCB Update\n",
    "        self.N_a[selected_model] += 1\n",
    "        \n",
    "        # Update empirical mean Q_t(a) (Incremental update formula)\n",
    "        n = self.N_a[selected_model]\n",
    "        old_q = self.Q_a[selected_model]\n",
    "        new_q = old_q + (1 / n) * (reward - old_q)\n",
    "        self.Q_a[selected_model] = new_q\n",
    "        \n",
    "        # 2. Calculate and update 'a' history for Myerson\n",
    "        exploration_term = self.c_ucb * math.sqrt(math.log(self.t) / self.N_a[selected_model])\n",
    "        a = reward + exploration_term - cost\n",
    "        self.a_history[selected_model].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b97883-c071-482d-9df7-22b0c5714a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting LLM Selection Simulation ---\n",
      "Models: ['Model-1', 'Model-2'], UCB-C: 0.5, Rounds: 200\n",
      "Max Reward per round: 70\n",
      "\n",
      "Round 20: Model=Model-2, R=60.00, C=8.70, Net=51.30\n",
      "Round 40: Model=Model-2, R=70.00, C=9.20, Net=60.80\n",
      "Round 60: Model=Model-2, R=35.00, C=9.30, Net=25.70\n",
      "Round 80: Model=Model-2, R=70.00, C=8.80, Net=61.20\n",
      "Round 100: Model=Model-2, R=25.00, C=9.00, Net=16.00\n",
      "Round 120: Model=Model-2, R=35.00, C=8.80, Net=26.20\n",
      "Round 140: Model=Model-2, R=35.00, C=8.70, Net=26.30\n",
      "Round 160: Model=Model-2, R=60.00, C=9.10, Net=50.90\n",
      "Round 180: Model=Model-2, R=10.00, C=9.20, Net=0.80\n",
      "Round 200: Model=Model-2, R=45.00, C=9.10, Net=35.90\n",
      "\n",
      "--- Simulation Complete ---\n",
      "Total Rounds: 200\n",
      "Total Revenue: 9560.00\n",
      "Total Cost: 2183.40\n",
      "Net Profit (Revenue - Cost): 7376.60\n",
      "\n",
      "--- Model Pull Counts ---\n",
      "Model-1: 8 pulls\n",
      "Model-2: 192 pulls\n",
      "\n",
      "--- Final UCB Mean Rewards (Q_a) ---\n",
      "Model-1: 64.3750 average reward\n",
      "Model-2: 47.1094 average reward\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(num_rounds: int, df: pd.DataFrame, selector: LLMBanditSelector):\n",
    "    \"\"\"Runs the multi-armed bandit simulation.\"\"\"\n",
    "    total_revenue = 0.0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    COUNTRY_LIST = df['Country'].tolist()\n",
    "\n",
    "    print(f\"--- Starting LLM Selection Simulation ---\")\n",
    "    print(f\"Models: {selector.models}, UCB-C: {selector.c_ucb}, Rounds: {num_rounds}\")\n",
    "    print(f\"Max Reward per round: {MAX_REWARD}\\n\")\n",
    "\n",
    "    for i in range(1, num_rounds + 1):\n",
    "        # 1. Choose a random country for the query\n",
    "        country = random.choice(COUNTRY_LIST)\n",
    "        \n",
    "        # 2. Select the optimal model\n",
    "        selected_model = selector.select_model(country)\n",
    "        \n",
    "        # 3. Simulate the performance (this is the actual *observation*)\n",
    "        reward, cost, tokens = simulate_llm_tokens_and_performance(country, selected_model) \n",
    "        \n",
    "        # 4. Update the selector state\n",
    "        selector.update_model_stats(selected_model, reward, cost)\n",
    "\n",
    "        total_revenue += reward\n",
    "        total_cost += cost\n",
    "        net_value = reward - cost\n",
    "        \n",
    "        if i % 20 == 0 or i == num_rounds:\n",
    "            print(f\"Round {i}: Model={selected_model}, R={reward:.2f}, C={cost:.2f}, Net={net_value:.2f}\")\n",
    "    print(\"\\n--- Simulation Complete ---\")\n",
    "    print(f\"Total Rounds: {num_rounds}\")\n",
    "    print(f\"Total Revenue: {total_revenue:.2f}\")\n",
    "    print(f\"Total Cost: {total_cost:.2f}\")\n",
    "    print(f\"Net Profit (Revenue - Cost): {total_revenue - total_cost:.2f}\")\n",
    "    \n",
    "    print(\"\\n--- Model Pull Counts ---\")\n",
    "    for model, count in selector.N_a.items():\n",
    "        print(f\"{model}: {count} pulls\")\n",
    "        \n",
    "    print(\"\\n--- Final UCB Mean Rewards (Q_a) ---\")\n",
    "    for model, q_a in selector.Q_a.items():\n",
    "        print(f\"{model}: {q_a:.4f} average reward\")\n",
    "        \n",
    "# --- Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # 1. Load the synthetic dataset\n",
    "        df_synthetic = pd.read_csv(DF_FILE_NAME)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: Dataset not found at {DF_FILE_NAME}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Initialize the LLM Selector\n",
    "    model_names = list(MODEL_CONFIG.keys())\n",
    "    bandit_selector = LLMBanditSelector(models=model_names, c_ucb=C_UCB)\n",
    "\n",
    "    # 3. Run the simulation\n",
    "    run_simulation(SIMULATION_ROUNDS, df_synthetic, bandit_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7bb503-a92f-4f64-a5f0-86b04292918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Starting Exclusive Simulation: Model-1 (200 Rounds) ---\n",
      "Round 50: Model=Model-1, R=70.00, C=57.60, Net=12.40\n",
      "Round 100: Model=Model-1, R=70.00, C=56.40, Net=13.60\n",
      "Round 150: Model=Model-1, R=70.00, C=57.60, Net=12.40\n",
      "Round 200: Model=Model-1, R=70.00, C=58.20, Net=11.80\n",
      "\n",
      "--- Exclusive Simulation Complete ---\n",
      "Model Used: Model-1\n",
      "Total Rounds: 200\n",
      "Total Revenue: 12230.00\n",
      "Total Cost: 11397.00\n",
      "Net Profit (Revenue - Cost): 833.00\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "--- Starting Exclusive Simulation: Model-2 (200 Rounds) ---\n",
      "Round 50: Model=Model-2, R=10.00, C=8.90, Net=1.10\n",
      "Round 100: Model=Model-2, R=70.00, C=9.20, Net=60.80\n",
      "Round 150: Model=Model-2, R=70.00, C=8.80, Net=61.20\n",
      "Round 200: Model=Model-2, R=70.00, C=9.10, Net=60.90\n",
      "\n",
      "--- Exclusive Simulation Complete ---\n",
      "Model Used: Model-2\n",
      "Total Rounds: 200\n",
      "Total Revenue: 9580.00\n",
      "Total Cost: 1798.50\n",
      "Net Profit (Revenue - Cost): 7781.50\n",
      "============================================================\n",
      "\n",
      "--- Final Comparative Profit ---\n",
      "Model-1 Exclusive Profit: 833.00\n",
      "Model-2 Exclusive Profit: 7781.50\n"
     ]
    }
   ],
   "source": [
    "# Assuming all constants (MODEL_CONFIG, REWARD_STRUCTURE, SIMULATION_ROUNDS,\n",
    "# DF_FILE_NAME, MAX_REWARD) and functions (simulate_llm_tokens_and_performance)\n",
    "# are already defined from the previous cell.\n",
    "\n",
    "def run_exclusive_model_simulation(num_rounds: int, df: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Runs a simulation for a fixed number of rounds using only the specified model\n",
    "    to calculate the total net profit.\n",
    "    \"\"\"\n",
    "    total_revenue = 0.0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    COUNTRY_LIST = df['Country'].tolist()\n",
    "\n",
    "    print(f\"--- Starting Exclusive Simulation: {model_name} ({num_rounds} Rounds) ---\")\n",
    "    \n",
    "    for i in range(1, num_rounds + 1):\n",
    "        # 1. Choose a random country for the query\n",
    "        country = random.choice(COUNTRY_LIST)\n",
    "        \n",
    "        # 2. Simulate the performance (This is the actual *observation*)\n",
    "        # The selected model is always the fixed model_name\n",
    "        reward, cost, tokens = simulate_llm_tokens_and_performance(country, model_name) \n",
    "        \n",
    "        total_revenue += reward\n",
    "        total_cost += cost\n",
    "        \n",
    "        if i % 50 == 0 or i == num_rounds:\n",
    "            net_value = reward - cost\n",
    "            print(f\"Round {i}: Model={model_name}, R={reward:.2f}, C={cost:.2f}, Net={net_value:.2f}\")\n",
    "\n",
    "    net_profit = total_revenue - total_cost\n",
    "    \n",
    "    print(\"\\n--- Exclusive Simulation Complete ---\")\n",
    "    print(f\"Model Used: {model_name}\")\n",
    "    print(f\"Total Rounds: {num_rounds}\")\n",
    "    print(f\"Total Revenue: {total_revenue:.2f}\")\n",
    "    print(f\"Total Cost: {total_cost:.2f}\")\n",
    "    print(f\"Net Profit (Revenue - Cost): {net_profit:.2f}\")\n",
    "    \n",
    "    return net_profit\n",
    "\n",
    "# --- Execution for Model-1 ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Load the synthetic dataset (assuming DF_FILE_NAME is globally accessible)\n",
    "        df_synthetic = pd.read_csv(DF_FILE_NAME)\n",
    "    except NameError:\n",
    "        print(\"Error: DF_FILE_NAME is not defined. Ensure you run the previous cell first.\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: Dataset not found at {DF_FILE_NAME}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Run simulation using only Model-1\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    profit_model_1 = run_exclusive_model_simulation(SIMULATION_ROUNDS, df_synthetic, 'Model-1')\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Optional: Run simulation using only Model-2 for comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    profit_model_2 = run_exclusive_model_simulation(SIMULATION_ROUNDS, df_synthetic, 'Model-2')\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n--- Final Comparative Profit ---\")\n",
    "    print(f\"Model-1 Exclusive Profit: {profit_model_1:.2f}\")\n",
    "    print(f\"Model-2 Exclusive Profit: {profit_model_2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012052e-c1f9-4d32-beec-93f17ce7b7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Llama Env)",
   "language": "python",
   "name": "my_llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
