{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66b76f-a4a1-4097-9925-4ac7e0c2e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama 3 pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6359e3ebda4427bf90bf6bd7258e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 pipeline loaded successfully.\n",
      "Starting evaluation of 196 countries. This may take some time...\n",
      "Processed 10/196 entries...\n",
      "Processed 20/196 entries...\n",
      "Processed 30/196 entries...\n",
      "Processed 40/196 entries...\n",
      "Processed 50/196 entries...\n",
      "Processed 60/196 entries...\n",
      "Processed 70/196 entries...\n",
      "Processed 80/196 entries...\n",
      "Processed 90/196 entries...\n",
      "Processed 100/196 entries...\n",
      "Processed 110/196 entries...\n",
      "Processed 120/196 entries...\n",
      "Processed 130/196 entries...\n",
      "Processed 140/196 entries...\n",
      "Processed 150/196 entries...\n",
      "Processed 160/196 entries...\n",
      "Processed 170/196 entries...\n",
      "Processed 180/196 entries...\n",
      "Processed 190/196 entries...\n",
      "Processed 196/196 entries...\n",
      "\n",
      "==================================================\n",
      "ðŸ§  Llama 3 Evaluation Results (Efficiency / Accuracy)\n",
      "==================================================\n",
      "Total Countries Evaluated: 196\n",
      "\n",
      "Accuracy by Column:\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 86.22%       |\n",
      "| Continent    | 91.84%       |\n",
      "| Latitude     | 19.39%       |\n",
      "| Longitude    | 18.88%       |\n",
      "\n",
      "Raw Counts:\n",
      "- Capital City: 169/196 correct\n",
      "- Continent: 180/196 correct\n",
      "- Latitude: 38/196 correct\n",
      "- Longitude: 37/196 correct\n"
     ]
    }
   ],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch\n",
    "# NOTE: The HuggingFace login needs to be run only once per session\n",
    "from huggingface_hub import login\n",
    "login(token='xxx') # Replace with your actual token line\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Suppress transformers warnings\n",
    "\n",
    "# --- 1. LLAMA 3 INITIALIZATION (THIS MUST RUN FIRST) ---\n",
    "try:\n",
    "    # Use your actual login token line here\n",
    "    # Assuming you have already run the login command successfully in your terminal/notebook\n",
    "    print(\"Initializing Llama 3 pipeline...\")\n",
    "    \n",
    "    model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    pipe_tokenizer = pipe.tokenizer\n",
    "    print(\"Llama 3 pipeline loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL: Failed to load Llama 3 pipeline. Check your environment, token, and hardware.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1) # Exit the script if the model cannot be loaded\n",
    "\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "QUERY_COLUMN = 'Country' \n",
    "EVAL_COLUMNS = ['Capital City', 'Continent', 'Latitude', 'Longitude'] \n",
    "\n",
    "\n",
    "# --- 3. UPDATED LLAMA 3 PREDICTION FUNCTION ---\n",
    "# This function now expects the 'pipe' and 'pipe_tokenizer' to be defined globally \n",
    "# by the initialization step above.\n",
    "def get_llama3_prediction(country_name, pipe, pipe_tokenizer):\n",
    "    \"\"\"\n",
    "    Takes a country name, queries the Llama 3 pipeline, and returns a dictionary.\n",
    "    \"\"\"\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle cases where the JSON is invalid\n",
    "            print(f\"Warning: Failed to parse JSON for {country_name}. Raw output: {raw_output[:50]}...\")\n",
    "            return {col: \"\" for col in EVAL_COLUMNS}\n",
    "    else:\n",
    "        # Handle cases where no JSON is found\n",
    "        # print(f\"Warning: No valid JSON found for {country_name}. Raw output: {raw_output[:50]}...\")\n",
    "        return {col: \"\" for col in EVAL_COLUMNS}\n",
    "\n",
    "\n",
    "# --- 4. EVALUATION LOGIC (Main loop) ---\n",
    "def calculate_efficiency(df, query_col, eval_cols, pipe, pipe_tokenizer):\n",
    "    \"\"\"Loops through the dataset, gets Llama 3 predictions, and calculates efficiency.\"\"\"\n",
    "    \n",
    "    for col in eval_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    correct_counts = {col: 0 for col in eval_cols}\n",
    "    total_count = len(df)\n",
    "\n",
    "    print(f\"Starting evaluation of {total_count} countries. This may take some time...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        \n",
    "        # Pass the initialized pipe and tokenizer objects\n",
    "        llm_response_dict = get_llama3_prediction(country, pipe, pipe_tokenizer)\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries...\")\n",
    "        \n",
    "        for col in eval_cols:\n",
    "            true_value = row[col]\n",
    "            predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "            \n",
    "            is_correct = (predicted_value == true_value)\n",
    "            \n",
    "            # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "            if col in ['Latitude', 'Longitude']:\n",
    "                true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                \n",
    "                try:\n",
    "                    true_num = float(true_num_str)\n",
    "                    pred_num = float(pred_num_str)\n",
    "                    \n",
    "                    if abs(true_num - pred_num) < 0.05:\n",
    "                         is_correct = True\n",
    "                    else:\n",
    "                        is_correct = False\n",
    "                except ValueError:\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_correct:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in eval_cols\n",
    "    }\n",
    "    \n",
    "    return efficiency, total_count, correct_counts\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "try:\n",
    "    # Load the ground truth data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Run the evaluation, passing the pipe and tokenizer\n",
    "    efficiency_results, total, correct = calculate_efficiency(data, QUERY_COLUMN, EVAL_COLUMNS, pipe, pipe_tokenizer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ§  Llama 3 Evaluation Results (Efficiency / Accuracy)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Countries Evaluated: {total}\")\n",
    "    print(\"\\nAccuracy by Column:\")\n",
    "    \n",
    "    results_table = pd.DataFrame([efficiency_results]).T\n",
    "    results_table.columns = ['Efficiency']\n",
    "    results_table.index.name = 'Column'\n",
    "    \n",
    "    print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "    print(\"\\nRaw Counts:\")\n",
    "    for col in EVAL_COLUMNS:\n",
    "        print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "    print(\"Please ensure the path is correct.\")\n",
    "except Exception as e:\n",
    "    # Catch any remaining errors outside of the loading and processing functions\n",
    "    print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bbfd2-b40b-4566-9dcb-b6988eb776d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama 3 pipeline...\n",
      "Llama 3 pipeline loaded successfully.\n",
      "Starting evaluation of 196 countries. This may take some time...\n",
      "Processed 10/196 entries...\n",
      "Processed 20/196 entries...\n",
      "Processed 30/196 entries...\n",
      "Processed 40/196 entries...\n",
      "Processed 50/196 entries...\n",
      "Processed 60/196 entries...\n",
      "Processed 70/196 entries...\n",
      "Processed 80/196 entries...\n",
      "Processed 90/196 entries...\n",
      "Warning: Failed to parse JSON for Peru. Raw output: {\"Capital City\": \"Lima\", \"Continent\": \"South Ameri...\n",
      "Processed 100/196 entries...\n",
      "Processed 110/196 entries...\n",
      "Processed 120/196 entries...\n",
      "Processed 130/196 entries...\n",
      "Processed 140/196 entries...\n",
      "Processed 150/196 entries...\n",
      "Processed 160/196 entries...\n",
      "Processed 170/196 entries...\n",
      "Processed 180/196 entries...\n",
      "Processed 190/196 entries...\n",
      "Processed 196/196 entries...\n",
      "\n",
      "==================================================\n",
      "ðŸ§  Llama 3 Evaluation Results (Efficiency / Accuracy)\n",
      "==================================================\n",
      "Total Countries Evaluated: 196\n",
      "\n",
      "Accuracy by Column:\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 83.67%       |\n",
      "| Continent    | 87.76%       |\n",
      "| Latitude     | 9.69%        |\n",
      "| Longitude    | 5.10%        |\n",
      "\n",
      "Raw Counts:\n",
      "- Capital City: 164/196 correct\n",
      "- Continent: 172/196 correct\n",
      "- Latitude: 19/196 correct\n",
      "- Longitude: 10/196 correct\n"
     ]
    }
   ],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch\n",
    "# NOTE: The HuggingFace login needs to be run only once per session\n",
    "from huggingface_hub import login\n",
    "login(token='xxx') # Replace with your actual token line\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Suppress transformers warnings\n",
    "\n",
    "# --- 1. LLAMA 3 INITIALIZATION (THIS MUST RUN FIRST) ---\n",
    "try:\n",
    "    # Use your actual login token line here\n",
    "    # Assuming you have already run the login command successfully in your terminal/notebook\n",
    "    print(\"Initializing Llama 3 pipeline...\")\n",
    "    \n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    pipe_tokenizer = pipe.tokenizer\n",
    "    print(\"Llama 3 pipeline loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL: Failed to load Llama 3 pipeline. Check your environment, token, and hardware.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1) # Exit the script if the model cannot be loaded\n",
    "\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "QUERY_COLUMN = 'Country' \n",
    "EVAL_COLUMNS = ['Capital City', 'Continent', 'Latitude', 'Longitude'] \n",
    "\n",
    "\n",
    "# --- 3. UPDATED LLAMA 3 PREDICTION FUNCTION ---\n",
    "# This function now expects the 'pipe' and 'pipe_tokenizer' to be defined globally \n",
    "# by the initialization step above.\n",
    "def get_llama3_prediction(country_name, pipe, pipe_tokenizer):\n",
    "    \"\"\"\n",
    "    Takes a country name, queries the Llama 3 pipeline, and returns a dictionary.\n",
    "    \"\"\"\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle cases where the JSON is invalid\n",
    "            print(f\"Warning: Failed to parse JSON for {country_name}. Raw output: {raw_output[:50]}...\")\n",
    "            return {col: \"\" for col in EVAL_COLUMNS}\n",
    "    else:\n",
    "        # Handle cases where no JSON is found\n",
    "        # print(f\"Warning: No valid JSON found for {country_name}. Raw output: {raw_output[:50]}...\")\n",
    "        return {col: \"\" for col in EVAL_COLUMNS}\n",
    "\n",
    "\n",
    "# --- 4. EVALUATION LOGIC (Main loop) ---\n",
    "def calculate_efficiency(df, query_col, eval_cols, pipe, pipe_tokenizer):\n",
    "    \"\"\"Loops through the dataset, gets Llama 3 predictions, and calculates efficiency.\"\"\"\n",
    "    \n",
    "    for col in eval_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    correct_counts = {col: 0 for col in eval_cols}\n",
    "    total_count = len(df)\n",
    "\n",
    "    print(f\"Starting evaluation of {total_count} countries. This may take some time...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        \n",
    "        # Pass the initialized pipe and tokenizer objects\n",
    "        llm_response_dict = get_llama3_prediction(country, pipe, pipe_tokenizer)\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries...\")\n",
    "        \n",
    "        for col in eval_cols:\n",
    "            true_value = row[col]\n",
    "            predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "            \n",
    "            is_correct = (predicted_value == true_value)\n",
    "            \n",
    "            # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "            if col in ['Latitude', 'Longitude']:\n",
    "                true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                \n",
    "                try:\n",
    "                    true_num = float(true_num_str)\n",
    "                    pred_num = float(pred_num_str)\n",
    "                    \n",
    "                    if abs(true_num - pred_num) < 0.05:\n",
    "                         is_correct = True\n",
    "                    else:\n",
    "                        is_correct = False\n",
    "                except ValueError:\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_correct:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in eval_cols\n",
    "    }\n",
    "    \n",
    "    return efficiency, total_count, correct_counts\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "try:\n",
    "    # Load the ground truth data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Run the evaluation, passing the pipe and tokenizer\n",
    "    efficiency_results, total, correct = calculate_efficiency(data, QUERY_COLUMN, EVAL_COLUMNS, pipe, pipe_tokenizer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ§  Llama 3 Evaluation Results (Efficiency / Accuracy)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Countries Evaluated: {total}\")\n",
    "    print(\"\\nAccuracy by Column:\")\n",
    "    \n",
    "    results_table = pd.DataFrame([efficiency_results]).T\n",
    "    results_table.columns = ['Efficiency']\n",
    "    results_table.index.name = 'Column'\n",
    "    \n",
    "    print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "    print(\"\\nRaw Counts:\")\n",
    "    for col in EVAL_COLUMNS:\n",
    "        print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "    print(\"Please ensure the path is correct.\")\n",
    "except Exception as e:\n",
    "    # Catch any remaining errors outside of the loading and processing functions\n",
    "    print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33ba0d-f485-4915-ba57-e36404f981f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbad3a2431743d3ab03d467aa8ed681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "Starting UCB/Reverse-Myerson evaluation on 196 countries...\n",
      "Processed 10/196 entries. Winner: LLM_3B | Total Reward: 19.00\n",
      "Processed 20/196 entries. Winner: LLM_1B | Total Reward: 39.00\n",
      "Processed 30/196 entries. Winner: LLM_3B | Total Reward: 59.00\n",
      "Processed 40/196 entries. Winner: LLM_1B | Total Reward: 78.00\n",
      "Processed 50/196 entries. Winner: LLM_3B | Total Reward: 100.00\n",
      "Processed 60/196 entries. Winner: LLM_1B | Total Reward: 117.00\n",
      "Processed 70/196 entries. Winner: LLM_3B | Total Reward: 139.00\n",
      "Processed 80/196 entries. Winner: LLM_3B | Total Reward: 158.00\n",
      "Processed 90/196 entries. Winner: LLM_3B | Total Reward: 174.00\n",
      "Processed 100/196 entries. Winner: LLM_3B | Total Reward: 191.00\n",
      "Processed 110/196 entries. Winner: LLM_3B | Total Reward: 208.00\n",
      "Processed 120/196 entries. Winner: LLM_1B | Total Reward: 226.00\n",
      "Processed 130/196 entries. Winner: LLM_3B | Total Reward: 244.00\n",
      "Processed 140/196 entries. Winner: LLM_1B | Total Reward: 258.00\n",
      "Processed 150/196 entries. Winner: LLM_3B | Total Reward: 279.00\n",
      "Processed 160/196 entries. Winner: LLM_3B | Total Reward: 295.00\n",
      "Processed 170/196 entries. Winner: LLM_1B | Total Reward: 318.00\n",
      "Processed 180/196 entries. Winner: LLM_1B | Total Reward: 335.00\n",
      "Processed 190/196 entries. Winner: LLM_3B | Total Reward: 357.00\n",
      "Processed 196/196 entries. Winner: LLM_3B | Total Reward: 366.00\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results\n",
      "======================================================================\n",
      "Total Countries Evaluated (t): 196\n",
      "Total Possible Reward: 1176\n",
      "Total Reward Collected: 366.00\n",
      "Total Cost Incurred (Mock): $0.06796050\n",
      "Net Utility (Reward - Cost): 365.93\n",
      "\n",
      "## Model Selection Counts\n",
      "| LLM    | Times Selected   |\n",
      "|:-------|:-----------------|\n",
      "| LLM_1B | 63               |\n",
      "| LLM_3B | 133              |\n",
      "\n",
      "## Final Accuracy (Based on Winning LLM's Prediction)\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 82.14%       |\n",
      "| Continent    | 87.24%       |\n",
      "| Latitude     | 4.59%        |\n",
      "| Longitude    | 4.08%        |\n",
      "\n",
      "Raw Correct Counts (for the selected winner):\n",
      "- Capital City: 161/196 correct\n",
      "- Continent: 171/196 correct\n",
      "- Latitude: 9/196 correct\n",
      "- Longitude: 8/196 correct\n",
      "\n",
      "LLM UCB Statistics:\n",
      "- LLM_1B: N=63, Mean Reward=1.6825\n",
      "- LLM_3B: N=133, Mean Reward=1.9549\n"
     ]
    }
   ],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch numpy scipy\n",
    "from huggingface_hub import login\n",
    "login(token='xxx') # Replace with your actual token line\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Suppress transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "LLM_CONFIGS = {\n",
    "    # Model ID, Name, and Mock Token Cost (for demonstration)\n",
    "    \"LLM_1B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"name\": \"1B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000005, # Mock cost: cheaper model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,    # N: Number of times this arm has been selected\n",
    "        \"ucb_Q\": 0.0,  # Q: Total reward received\n",
    "        \"ucb_mean_reward\": 0.0, # Q/N: Average reward\n",
    "    },\n",
    "    \"LLM_3B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"name\": \"3B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000025, # Mock cost: more expensive model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,\n",
    "        \"ucb_Q\": 0.0,\n",
    "        \"ucb_mean_reward\": 0.0,\n",
    "    }\n",
    "}\n",
    "# UCB exploration parameter\n",
    "UCB_C = 0.5\n",
    "\n",
    "# Mapping columns to rewards for a correct prediction\n",
    "REWARD_MAP = {\n",
    "    'Capital City': 1,\n",
    "    'Continent': 1,\n",
    "    'Latitude': 2,\n",
    "    'Longitude': 2\n",
    "}\n",
    "EVAL_COLUMNS = list(REWARD_MAP.keys())\n",
    "\n",
    "# Dataset configuration\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "#file_path = 'all capital cities in the world.csv' # Placeholder for a common file structure\n",
    "QUERY_COLUMN = 'Country'\n",
    "\n",
    "# Global counter for the total number of rounds (t in UCB)\n",
    "GLOBAL_T = 0\n",
    "\n",
    "# --- 1. LLM INITIALIZATION ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initializes both Llama 3 models.\"\"\"\n",
    "    global LLM_CONFIGS\n",
    "    print(\"Initializing LLM pipelines...\")\n",
    "    \n",
    "    for key, config in LLM_CONFIGS.items():\n",
    "        try:\n",
    "            print(f\"Loading {config['name']} ({config['model_id']})...\")\n",
    "            # Using low-precision dtype and device_map requires 'accelerate'\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=config['model_id'],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            config[\"pipe\"] = pipe\n",
    "            config[\"tokenizer\"] = pipe.tokenizer\n",
    "            print(f\"{config['name']} loaded successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL: Failed to load {config['name']} pipeline. Check environment, token, and hardware.\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# --- 2. LLM PREDICTION AND COST CALCULATION ---\n",
    "def get_llm_prediction_and_cost(country_name, llm_key):\n",
    "    \"\"\"\n",
    "    Queries the specified Llama 3 pipeline, returns data, raw output, and mock cost.\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    pipe = config[\"pipe\"]\n",
    "    pipe_tokenizer = config[\"tokenizer\"]\n",
    "    cost_per_token = config[\"cost_per_token\"]\n",
    "\n",
    "    # 2.1 Construct Prompt\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template for Llama 3 format\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 2.2 Run Inference\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 2.3 Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    llm_response_dict = {col: \"\" for col in EVAL_COLUMNS}\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            llm_response_dict = json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            pass # Keep default empty dict if parsing fails\n",
    "\n",
    "    # 2.4 Mock Cost Calculation\n",
    "    # NOTE: In a real scenario, this would use the model API's token count.\n",
    "    # Here we use a mock count based on character length for demonstration.\n",
    "    \n",
    "    # Token count estimation: 1 token is roughly 4 characters\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    response_tokens = len(raw_output) // 4\n",
    "    total_tokens = prompt_tokens + response_tokens\n",
    "    \n",
    "    cost = total_tokens * cost_per_token\n",
    "\n",
    "    return llm_response_dict, raw_output, total_tokens, cost\n",
    "\n",
    "\n",
    "# --- 3. UCB AND MYERSON LOGIC ---\n",
    "\n",
    "def calculate_ucb_score(llm_key, current_t):\n",
    "    \"\"\"Calculates the Upper Confidence Bound (UCB) score for an LLM arm.\"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Initialize UCB statistics if N=0 (to ensure every arm is selected at least once)\n",
    "    if config[\"ucb_N\"] == 0:\n",
    "        return float('inf') \n",
    "    \n",
    "    # Mean reward (Exploitation component)\n",
    "    exploitation_term = config[\"ucb_mean_reward\"]\n",
    "    \n",
    "    # Exploration component: C * sqrt(ln(t) / N)\n",
    "    exploration_term = UCB_C * math.sqrt(math.log(current_t) / config[\"ucb_N\"])\n",
    "    \n",
    "    ucb_score = exploitation_term + exploration_term\n",
    "    return ucb_score\n",
    "\n",
    "def calculate_virtual_valuation(llm_key, country_name, current_t, total_reward, total_cost):\n",
    "    \"\"\"\n",
    "    Calculates the Virtual Valuation for a given LLM's result using a modified\n",
    "    Reverse-Myerson approach: V = a + CDF(a)/PDF(a).\n",
    "\n",
    "    a = Reward + C * sqrt(ln(t)/N) - Cost\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Step 1: Calculate 'a' (Adjusted Utility/Bid)\n",
    "    # The term C * sqrt(ln(t)/N) is the UCB exploration bonus.\n",
    "    \n",
    "    # If N is 0, initialize N to 1 to avoid division by zero and give a high 'a'\n",
    "    N_eff = max(config[\"ucb_N\"], 1)\n",
    "    \n",
    "    # This is the UCB score (Exploitation + Exploration)\n",
    "    ucb_component = config[\"ucb_mean_reward\"] + UCB_C * math.sqrt(math.log(current_t) / N_eff)\n",
    "\n",
    "    # We use the *current* reward in the calculation, not the average, \n",
    "    # to evaluate the specific outcome.\n",
    "    a = total_reward + UCB_C * math.sqrt(math.log(current_t) / N_eff) - total_cost\n",
    "    \n",
    "    # Step 2: Calculate CDF(a) and PDF(a)\n",
    "    # We use a standard normal distribution (mean=0, std=1) for simplicity.\n",
    "    # In a real auction, these functions would be derived from the *prior distribution* # of the model's reward-minus-cost utility.\n",
    "    \n",
    "    try:\n",
    "        # Calculate Probability Density Function (PDF)\n",
    "        pdf_a = norm.pdf(a)\n",
    "        \n",
    "        # Calculate Cumulative Distribution Function (CDF)\n",
    "        cdf_a = norm.cdf(a)\n",
    "    except ValueError:\n",
    "        # Catch edge cases where 'a' is extreme\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "    # Step 3: Calculate Virtual Valuation (Reverse-Myerson)\n",
    "    # The winner is the one with the *lowest* virtual valuation.\n",
    "    # V(a) = a + CDF(a) / PDF(a)\n",
    "    \n",
    "    if pdf_a == 0:\n",
    "        # This happens for extreme 'a' values. Assign a massive valuation.\n",
    "        virtual_valuation = float('inf')\n",
    "    else:\n",
    "        virtual_valuation = a + (cdf_a / pdf_a)\n",
    "        \n",
    "    return virtual_valuation\n",
    "\n",
    "def update_ucb_stats(llm_key, reward):\n",
    "    \"\"\"Updates the UCB statistics for the winning LLM arm.\"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Update total reward (Q) and count (N)\n",
    "    config[\"ucb_Q\"] += reward\n",
    "    config[\"ucb_N\"] += 1\n",
    "    \n",
    "    # Update mean reward\n",
    "    config[\"ucb_mean_reward\"] = config[\"ucb_Q\"] / config[\"ucb_N\"]\n",
    "\n",
    "\n",
    "# --- 4. EVALUATION LOGIC (Main loop) ---\n",
    "def calculate_efficiency_with_ucb_myerson(df, query_col):\n",
    "    \"\"\"\n",
    "    Loops through the dataset, gets predictions from both LLMs, \n",
    "    applies UCB/Myerson logic to select the winner, and calculates overall efficiency.\n",
    "    \"\"\"\n",
    "    global GLOBAL_T\n",
    "    \n",
    "    # Data cleaning for ground truth\n",
    "    for col in EVAL_COLUMNS:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    total_count = len(df)\n",
    "    \n",
    "    # Tracking for final results\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    llm_selection_counts = {key: 0 for key in LLM_CONFIGS.keys()}\n",
    "    total_reward_collected = 0\n",
    "    total_cost_incurred = 0\n",
    "\n",
    "    print(f\"Starting UCB/Reverse-Myerson evaluation on {total_count} countries...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        GLOBAL_T += 1 # Increment total rounds (t)\n",
    "\n",
    "        # Dictionary to hold the results for both LLMs in this round\n",
    "        round_results = {}\n",
    "        \n",
    "        # 4.1 Get Predictions, Rewards, and Costs for BOTH LLMs\n",
    "        for llm_key in LLM_CONFIGS.keys():\n",
    "            # Run inference for the current LLM\n",
    "            llm_response_dict, raw_output, total_tokens, cost = \\\n",
    "                get_llm_prediction_and_cost(country, llm_key)\n",
    "            \n",
    "            # Calculate Total Reward for this LLM's prediction\n",
    "            current_reward = 0\n",
    "            is_correct_for_llm = {col: False for col in EVAL_COLUMNS}\n",
    "            \n",
    "            for col in EVAL_COLUMNS:\n",
    "                true_value = row[col]\n",
    "                predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "                \n",
    "                is_correct = (predicted_value == true_value)\n",
    "                \n",
    "                # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "                if col in ['Latitude', 'Longitude']:\n",
    "                    true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                    pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                    \n",
    "                    try:\n",
    "                        true_num = float(true_num_str)\n",
    "                        pred_num = float(pred_num_str)\n",
    "                        \n",
    "                        if abs(true_num - pred_num) < 0.05:\n",
    "                            is_correct = True\n",
    "                        else:\n",
    "                            is_correct = False\n",
    "                    except ValueError:\n",
    "                        is_correct = False\n",
    "                \n",
    "                if is_correct:\n",
    "                    current_reward += REWARD_MAP[col]\n",
    "                    is_correct_for_llm[col] = True\n",
    "            \n",
    "            round_results[llm_key] = {\n",
    "                \"reward\": current_reward,\n",
    "                \"cost\": cost,\n",
    "                \"is_correct\": is_correct_for_llm,\n",
    "                \"response_dict\": llm_response_dict\n",
    "            }\n",
    "\n",
    "\n",
    "        # 4.2 UCB and Reverse-Myerson Selection\n",
    "        \n",
    "        # 1. Calculate Virtual Valuation for each LLM's result\n",
    "        virtual_valuations = {}\n",
    "        for llm_key, result in round_results.items():\n",
    "            virtual_valuations[llm_key] = calculate_virtual_valuation(\n",
    "                llm_key, country, GLOBAL_T, result[\"reward\"], result[\"cost\"]\n",
    "            )\n",
    "\n",
    "        # 2. Select the winner: Lowest Virtual Valuation wins (Reverse-Myerson)\n",
    "        winning_llm_key = min(virtual_valuations, key=virtual_valuations.get)\n",
    "        \n",
    "        winning_result = round_results[winning_llm_key]\n",
    "        \n",
    "        # 4.3 Update Statistics\n",
    "        \n",
    "        # Update UCB/Myerson Arm Stats\n",
    "        update_ucb_stats(winning_llm_key, winning_result[\"reward\"])\n",
    "        llm_selection_counts[winning_llm_key] += 1\n",
    "        \n",
    "        # Update Overall Evaluation Metrics\n",
    "        total_reward_collected += winning_result[\"reward\"]\n",
    "        total_cost_incurred += winning_result[\"cost\"]\n",
    "        \n",
    "        for col in EVAL_COLUMNS:\n",
    "            if winning_result[\"is_correct\"][col]:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries. Winner: {winning_llm_key} | Total Reward: {total_reward_collected:.2f}\")\n",
    "\n",
    "    # 4.4 Final Efficiency Calculation\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "    \n",
    "    total_possible_reward = total_count * sum(REWARD_MAP.values())\n",
    "    \n",
    "    return (efficiency, total_count, correct_counts, llm_selection_counts, \n",
    "            total_reward_collected, total_possible_reward, total_cost_incurred)\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 5.1 Initialize LLMs\n",
    "    initialize_llms()\n",
    "    \n",
    "    # 5.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = calculate_efficiency_with_ucb_myerson(data, QUERY_COLUMN)\n",
    "        (efficiency_results, total, correct, llm_selections, \n",
    "         total_reward, total_possible_reward, total_cost) = results\n",
    "        \n",
    "        # 5.3 Print Final Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Countries Evaluated (t): {total}\")\n",
    "        print(f\"Total Possible Reward: {total_possible_reward}\")\n",
    "        print(f\"Total Reward Collected: {total_reward:.2f}\")\n",
    "        print(f\"Total Cost Incurred (Mock): ${total_cost:.8f}\")\n",
    "        print(f\"Net Utility (Reward - Cost): {total_reward - total_cost:.2f}\")\n",
    "        \n",
    "        print(\"\\n## Model Selection Counts\")\n",
    "        selection_table = pd.DataFrame([llm_selections]).T\n",
    "        selection_table.columns = ['Times Selected']\n",
    "        selection_table.index.name = 'LLM'\n",
    "        print(selection_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "        \n",
    "        print(\"\\n## Final Accuracy (Based on Winning LLM's Prediction)\")\n",
    "        results_table = pd.DataFrame([efficiency_results]).T\n",
    "        results_table.columns = ['Efficiency']\n",
    "        results_table.index.name = 'Column'\n",
    "        print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        print(\"\\nRaw Correct Counts (for the selected winner):\")\n",
    "        for col in EVAL_COLUMNS:\n",
    "            print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "        \n",
    "        print(\"\\nLLM UCB Statistics:\")\n",
    "        for key, config in LLM_CONFIGS.items():\n",
    "             print(f\"- {key}: N={config['ucb_N']}, Mean Reward={config['ucb_mean_reward']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81fc2c1c-b6eb-4d1e-8c3f-030cb3f99b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines for single-LLM utility calculation...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086d00db75e4b37916525e7024e66be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "\n",
      "ðŸš€ Starting single-LLM evaluation for 1B-Instruct on 196 countries...\n",
      "Processed 10/196 entries...\n",
      "Processed 20/196 entries...\n",
      "Processed 30/196 entries...\n",
      "Processed 40/196 entries...\n",
      "Processed 50/196 entries...\n",
      "Processed 60/196 entries...\n",
      "Processed 70/196 entries...\n",
      "Processed 80/196 entries...\n",
      "Processed 90/196 entries...\n",
      "Processed 100/196 entries...\n",
      "Processed 110/196 entries...\n",
      "Processed 120/196 entries...\n",
      "Processed 130/196 entries...\n",
      "Processed 140/196 entries...\n",
      "Processed 150/196 entries...\n",
      "Processed 160/196 entries...\n",
      "Processed 170/196 entries...\n",
      "Processed 180/196 entries...\n",
      "Processed 190/196 entries...\n",
      "Processed 196/196 entries...\n",
      "\n",
      "ðŸš€ Starting single-LLM evaluation for 3B-Instruct on 196 countries...\n",
      "Processed 10/196 entries...\n",
      "Processed 20/196 entries...\n",
      "Processed 30/196 entries...\n",
      "Processed 40/196 entries...\n",
      "Processed 50/196 entries...\n",
      "Processed 60/196 entries...\n",
      "Processed 70/196 entries...\n",
      "Processed 80/196 entries...\n",
      "Processed 90/196 entries...\n",
      "Processed 100/196 entries...\n",
      "Processed 110/196 entries...\n",
      "Processed 120/196 entries...\n",
      "Processed 130/196 entries...\n",
      "Processed 140/196 entries...\n",
      "Processed 150/196 entries...\n",
      "Processed 160/196 entries...\n",
      "Processed 170/196 entries...\n",
      "Processed 180/196 entries...\n",
      "Processed 190/196 entries...\n",
      "Processed 196/196 entries...\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š Single-LLM Baseline Utility Comparison\n",
      "======================================================================\n",
      "|    | Metric                      | 1B-Instruct   | 3B-Instruct   |\n",
      "|:---|:----------------------------|:--------------|:--------------|\n",
      "| 0  | Total Reward                | 394.00        | 499.00        |\n",
      "| 1  | Total Cost (Mock)           | $0.01818050   | $0.09151250   |\n",
      "| 2  | Net Utility (Reward - Cost) | 393.98        | 498.91        |\n",
      "\n",
      "## Accuracy by LLM\n",
      "|    | Column       | 1B-Instruct (%)   | 3B-Instruct (%)   |\n",
      "|:---|:-------------|:------------------|:------------------|\n",
      "| 0  | Capital City | 83.67%            | 86.22%            |\n",
      "| 1  | Continent    | 87.76%            | 91.84%            |\n",
      "| 2  | Latitude     | 9.69%             | 19.39%            |\n",
      "| 3  | Longitude    | 5.10%             | 18.88%            |\n"
     ]
    }
   ],
   "source": [
    "# --- 0. IMPORTS AND SETUP (Re-use existing setup) ---\n",
    "# Assuming you have run the initial setup, including pip installs and huggingface login.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR) # Suppress transformers warnings\n",
    "\n",
    "# --- GLOBAL CONFIGURATION (Must match the previous script) ---\n",
    "LLM_CONFIGS = {\n",
    "    # Model ID, Name, and Mock Token Cost\n",
    "    \"LLM_1B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"name\": \"1B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000005, # Mock cost: cheaper model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "    },\n",
    "    \"LLM_3B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"name\": \"3B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000025, # Mock cost: more expensive model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mapping columns to rewards for a correct prediction\n",
    "REWARD_MAP = {\n",
    "    'Capital City': 1,\n",
    "    'Continent': 1,\n",
    "    'Latitude': 2,\n",
    "    'Longitude': 2\n",
    "}\n",
    "EVAL_COLUMNS = list(REWARD_MAP.keys())\n",
    "\n",
    "# Dataset configuration\n",
    "# !!! IMPORTANT: REPLACE THIS PATH WITH YOUR ACTUAL FILE PATH !!!\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv' \n",
    "QUERY_COLUMN = 'Country'\n",
    "\n",
    "# --- 1. LLM INITIALIZATION (Modified to only initialize, pipes stored in CONFIG) ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initializes both Llama 3 models.\"\"\"\n",
    "    global LLM_CONFIGS\n",
    "    print(\"Initializing LLM pipelines for single-LLM utility calculation...\")\n",
    "    \n",
    "    for key, config in LLM_CONFIGS.items():\n",
    "        try:\n",
    "            print(f\"Loading {config['name']} ({config['model_id']})...\")\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=config['model_id'],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            config[\"pipe\"] = pipe\n",
    "            config[\"tokenizer\"] = pipe.tokenizer\n",
    "            print(f\"{config['name']} loaded successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL: Failed to load {config['name']} pipeline. Error: {e}\")\n",
    "            # Do not exit, but mark as unusable\n",
    "            config[\"pipe\"] = None \n",
    "            config[\"tokenizer\"] = None\n",
    "\n",
    "\n",
    "# --- 2. LLM PREDICTION AND COST CALCULATION (Reused from previous code) ---\n",
    "def get_llm_prediction_and_cost(country_name, llm_key):\n",
    "    \"\"\"\n",
    "    Queries the specified Llama 3 pipeline, returns data, raw output, and mock cost.\n",
    "    This function is identical to the one in the previous script.\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    pipe = config[\"pipe\"]\n",
    "    pipe_tokenizer = config[\"tokenizer\"]\n",
    "    cost_per_token = config[\"cost_per_token\"]\n",
    "\n",
    "    if pipe is None:\n",
    "        return {col: \"\" for col in EVAL_COLUMNS}, \"\", 0, float('inf')\n",
    "\n",
    "    # 2.1 Construct Prompt\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 2.2 Run Inference\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 2.3 Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    llm_response_dict = {col: \"\" for col in EVAL_COLUMNS}\n",
    "    \n",
    "    if json_match:\n",
    "        try:\n",
    "            llm_response_dict = json.loads(json_match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # 2.4 Mock Cost Calculation\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    response_tokens = len(raw_output) // 4\n",
    "    total_tokens = prompt_tokens + response_tokens\n",
    "    cost = total_tokens * cost_per_token\n",
    "\n",
    "    return llm_response_dict, raw_output, total_tokens, cost\n",
    "\n",
    "\n",
    "# --- 3. SINGLE LLM UTILITY CALCULATION FUNCTION ---\n",
    "def calculate_single_llm_utility(df_original, query_col, llm_key):\n",
    "    \"\"\"\n",
    "    Loops through the dataset using only the specified LLM, calculates \n",
    "    total reward, total cost, and net utility (reward - cost).\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to clean the ground truth values\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    # Data cleaning for ground truth\n",
    "    for col in EVAL_COLUMNS:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    total_count = len(df)\n",
    "    total_reward = 0\n",
    "    total_cost = 0\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    \n",
    "    llm_name = LLM_CONFIGS[llm_key]['name']\n",
    "    print(f\"\\nðŸš€ Starting single-LLM evaluation for {llm_name} on {total_count} countries...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        \n",
    "        # Get prediction and cost\n",
    "        llm_response_dict, _, _, cost = \\\n",
    "            get_llm_prediction_and_cost(country, llm_key)\n",
    "        \n",
    "        total_cost += cost\n",
    "        current_reward = 0\n",
    "        \n",
    "        # Calculate Reward based on accuracy\n",
    "        for col in EVAL_COLUMNS:\n",
    "            true_value = row[col]\n",
    "            predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "            \n",
    "            is_correct = (predicted_value == true_value)\n",
    "            \n",
    "            # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "            if col in ['Latitude', 'Longitude']:\n",
    "                true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                \n",
    "                try:\n",
    "                    true_num = float(true_num_str)\n",
    "                    pred_num = float(pred_num_str)\n",
    "                    \n",
    "                    if abs(true_num - pred_num) < 0.05:\n",
    "                        is_correct = True\n",
    "                    else:\n",
    "                        is_correct = False\n",
    "                except ValueError:\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_correct:\n",
    "                current_reward += REWARD_MAP[col]\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        total_reward += current_reward\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries...\")\n",
    "\n",
    "    # Final Efficiency Calculation\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "    \n",
    "    net_utility = total_reward - total_cost\n",
    "    \n",
    "    return {\n",
    "        \"LLM_Name\": llm_name,\n",
    "        \"Total_Reward\": total_reward,\n",
    "        \"Total_Cost\": total_cost,\n",
    "        \"Net_Utility\": net_utility,\n",
    "        \"Accuracy\": efficiency\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 4.1 Initialize LLMs\n",
    "    initialize_llms()\n",
    "    \n",
    "    # 4.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        results_llm1 = calculate_single_llm_utility(data, QUERY_COLUMN, \"LLM_1B\")\n",
    "        results_llm2 = calculate_single_llm_utility(data, QUERY_COLUMN, \"LLM_3B\")\n",
    "        \n",
    "        # 4.3 Print Comparison Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ“Š Single-LLM Baseline Utility Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        comparison_data = {\n",
    "            \"Metric\": [\"Total Reward\", \"Total Cost (Mock)\", \"Net Utility (Reward - Cost)\"],\n",
    "            results_llm1['LLM_Name']: [\n",
    "                f\"{results_llm1['Total_Reward']:.2f}\", \n",
    "                f\"${results_llm1['Total_Cost']:.8f}\", \n",
    "                f\"{results_llm1['Net_Utility']:.2f}\"\n",
    "            ],\n",
    "            results_llm2['LLM_Name']: [\n",
    "                f\"{results_llm2['Total_Reward']:.2f}\", \n",
    "                f\"${results_llm2['Total_Cost']:.8f}\", \n",
    "                f\"{results_llm2['Net_Utility']:.2f}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        comparison_table = pd.DataFrame(comparison_data)\n",
    "        print(comparison_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "        \n",
    "        print(\"\\n## Accuracy by LLM\")\n",
    "        accuracy_data = {\n",
    "            \"Column\": list(results_llm1['Accuracy'].keys()),\n",
    "            results_llm1['LLM_Name'] + ' (%)': list(results_llm1['Accuracy'].values()),\n",
    "            results_llm2['LLM_Name'] + ' (%)': list(results_llm2['Accuracy'].values())\n",
    "        }\n",
    "        accuracy_table = pd.DataFrame(accuracy_data)\n",
    "        print(accuracy_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c876dac-8725-4633-b025-6ff78bf7954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc01eebd172c4382862e21bfe90290c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "Starting UCB/Reverse-Myerson evaluation on 196 countries...\n",
      "Processed 10/196 entries. Winner: LLM_3B | Total Reward: 28.00\n",
      "Processed 20/196 entries. Winner: LLM_3B | Total Reward: 54.00\n",
      "Processed 30/196 entries. Winner: LLM_1B | Total Reward: 78.00\n",
      "Processed 40/196 entries. Winner: LLM_3B | Total Reward: 105.00\n",
      "Processed 50/196 entries. Winner: LLM_3B | Total Reward: 135.00\n",
      "Processed 60/196 entries. Winner: LLM_3B | Total Reward: 162.00\n",
      "Processed 70/196 entries. Winner: LLM_1B | Total Reward: 191.00\n",
      "Processed 80/196 entries. Winner: LLM_3B | Total Reward: 218.00\n",
      "Processed 90/196 entries. Winner: LLM_3B | Total Reward: 242.00\n",
      "Processed 100/196 entries. Winner: LLM_3B | Total Reward: 270.00\n",
      "Processed 110/196 entries. Winner: LLM_1B | Total Reward: 291.00\n",
      "Processed 120/196 entries. Winner: LLM_3B | Total Reward: 320.00\n",
      "Processed 130/196 entries. Winner: LLM_1B | Total Reward: 345.00\n",
      "Processed 140/196 entries. Winner: LLM_3B | Total Reward: 370.00\n",
      "Processed 150/196 entries. Winner: LLM_3B | Total Reward: 400.00\n",
      "Processed 160/196 entries. Winner: LLM_1B | Total Reward: 425.00\n",
      "Processed 170/196 entries. Winner: LLM_3B | Total Reward: 458.00\n",
      "Processed 180/196 entries. Winner: LLM_3B | Total Reward: 483.00\n",
      "Processed 190/196 entries. Winner: LLM_1B | Total Reward: 515.00\n",
      "Processed 196/196 entries. Winner: LLM_3B | Total Reward: 527.00\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (FIXED)\n",
      "======================================================================\n",
      "Total Countries Evaluated (t): 196\n",
      "Total Possible Reward: 1176\n",
      "Total Reward Collected: 527.00\n",
      "Total Cost Incurred (Mock): $0.05484450\n",
      "Net Utility (Reward - Cost): 526.95\n",
      "\n",
      "## Model Selection Counts\n",
      "| LLM    | Times Selected   |\n",
      "|:-------|:-----------------|\n",
      "| LLM_1B | 98               |\n",
      "| LLM_3B | 98               |\n",
      "\n",
      "## Final Accuracy (Based on Winning LLM's Prediction)\n",
      "| Column       | Efficiency   |\n",
      "|:-------------|:-------------|\n",
      "| Capital City | 87.76%       |\n",
      "| Continent    | 92.35%       |\n",
      "| Latitude     | 24.49%       |\n",
      "| Longitude    | 19.90%       |\n",
      "\n",
      "Raw Correct Counts (for the selected winner):\n",
      "- Capital City: 172/196 correct\n",
      "- Continent: 181/196 correct\n",
      "- Latitude: 48/196 correct\n",
      "- Longitude: 39/196 correct\n",
      "\n",
      "LLM UCB Statistics (Mean Reward = Mean Utility):\n",
      "- LLM_1B: N=98, Mean Utility=2.2754\n",
      "- LLM_3B: N=98, Mean Utility=3.1016\n"
     ]
    }
   ],
   "source": [
    "# --- 0. INSTALL AND SETUP (Run these lines first if you haven't already) ---\n",
    "# !pip install accelerate transformers pandas torch numpy scipy\n",
    "from huggingface_hub import login\n",
    "# login(token='use your token') # Replace with your actual token line\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Suppress transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "LLM_CONFIGS = {\n",
    "    # Model ID, Name, and Mock Token Cost (for demonstration)\n",
    "    \"LLM_1B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"name\": \"1B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000005, # Mock cost: cheaper model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,    # N: Number of times this arm has been selected\n",
    "        \"ucb_Q\": 0.0,  # Q: Total utility (Reward - Cost) received\n",
    "        \"ucb_mean_reward\": 0.0, # Q/N: Average utility (Note: Key kept as 'reward' for simplicity)\n",
    "    },\n",
    "    \"LLM_3B\": {\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"name\": \"3B-Instruct\",\n",
    "        \"cost_per_token\": 0.0000025, # Mock cost: more expensive model\n",
    "        \"pipe\": None,\n",
    "        \"tokenizer\": None,\n",
    "        \"ucb_N\": 0,\n",
    "        \"ucb_Q\": 0.0,\n",
    "        \"ucb_mean_reward\": 0.0,\n",
    "    }\n",
    "}\n",
    "# UCB exploration parameter\n",
    "UCB_C = 0.5\n",
    "\n",
    "# Mapping columns to rewards for a correct prediction\n",
    "REWARD_MAP = {\n",
    "    'Capital City': 1,\n",
    "    'Continent': 1,\n",
    "    'Latitude': 2,\n",
    "    'Longitude': 2\n",
    "}\n",
    "EVAL_COLUMNS = list(REWARD_MAP.keys())\n",
    "\n",
    "# Dataset configuration\n",
    "# IMPORTANT: Use the exact path to your CSV file\n",
    "file_path = '/home/gella.saikrishna/.cache/kagglehub/datasets/dataanalyst001/all-capital-cities-in-the-world/versions/1/all capital cities in the world.csv'\n",
    "#file_path = 'all capital cities in the world.csv' # Placeholder for a common file structure\n",
    "QUERY_COLUMN = 'Country'\n",
    "\n",
    "# Global counter for the total number of rounds (t in UCB)\n",
    "GLOBAL_T = 0\n",
    "\n",
    "# --- 1. LLM INITIALIZATION ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initializes both Llama 3 models.\"\"\"\n",
    "    global LLM_CONFIGS\n",
    "    print(\"Initializing LLM pipelines...\")\n",
    "    \n",
    "    for key, config in LLM_CONFIGS.items():\n",
    "        try:\n",
    "            print(f\"Loading {config['name']} ({config['model_id']})...\")\n",
    "            # Using low-precision dtype and device_map requires 'accelerate'\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=config['model_id'],\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            config[\"pipe\"] = pipe\n",
    "            config[\"tokenizer\"] = pipe.tokenizer\n",
    "            print(f\"{config['name']} loaded successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFATAL: Failed to load {config['name']} pipeline. Check environment, token, and hardware.\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# --- 2. LLM PREDICTION AND COST CALCULATION ---\n",
    "def get_llm_prediction_and_cost(country_name, llm_key):\n",
    "    \"\"\"\n",
    "    Queries the specified Llama 3 pipeline, returns data, raw output, and mock cost.\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    pipe = config[\"pipe\"]\n",
    "    pipe_tokenizer = config[\"tokenizer\"]\n",
    "    cost_per_token = config[\"cost_per_token\"]\n",
    "\n",
    "    # 2.1 Construct Prompt\n",
    "    prompt_instruction = f\"\"\"\n",
    "    You are an expert geographical information system. \n",
    "    Your task is to provide the Capital City, Continent, Latitude, and Longitude for the requested country.\n",
    "    You MUST respond ONLY with a valid JSON object. DO NOT include any text outside the JSON object.\n",
    "    The JSON structure must be: {{\"Capital City\": \"...\", \"Continent\": \"...\", \"Latitude\": \"...\", \"Longitude\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide the geographical data for: {country_name}\"},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template for Llama 3 format\n",
    "    prompt = pipe_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 2.2 Run Inference\n",
    "    terminators = [\n",
    "        pipe_tokenizer.eos_token_id,\n",
    "        pipe_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run Inference with deterministic settings\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 2.3 Extract and Parse the JSON\n",
    "    raw_output = outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
    "    \n",
    "    llm_response_dict = {col: \"\" for col in EVAL_COLUMNS}\n",
    "    \n",
    "    if json_match:\n",
    "        json_string = json_match.group(0)\n",
    "        try:\n",
    "            llm_response_dict = json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            pass # Keep default empty dict if parsing fails\n",
    "\n",
    "    # 2.4 Mock Cost Calculation\n",
    "    # Token count estimation: 1 token is roughly 4 characters\n",
    "    prompt_tokens = len(prompt) // 4\n",
    "    response_tokens = len(raw_output) // 4\n",
    "    total_tokens = prompt_tokens + response_tokens\n",
    "    \n",
    "    cost = total_tokens * cost_per_token\n",
    "\n",
    "    return llm_response_dict, raw_output, total_tokens, cost\n",
    "\n",
    "\n",
    "# --- 3. UCB AND MYERSON LOGIC (CORRECTED) ---\n",
    "\n",
    "def calculate_virtual_valuation(llm_key, country_name, current_t, total_reward, total_cost):\n",
    "    \"\"\"\n",
    "    Calculates the Virtual Valuation for a given LLM's result.\n",
    "    The winner will be the one with the *HIGHEST* Virtual Valuation to maximize utility.\n",
    "\n",
    "    a = Reward - Cost + C * sqrt(ln(t)/N) (Adjusted Utility/Bid)\n",
    "    \"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Step 1: Calculate 'a' (Adjusted Utility/Bid)\n",
    "    N_eff = max(config[\"ucb_N\"], 1)\n",
    "    \n",
    "    # 'a' is the current Utility (Reward - Cost) + Exploration Bonus\n",
    "    a = total_reward - total_cost + UCB_C * math.sqrt(math.log(current_t) / N_eff)\n",
    "    \n",
    "    # Step 2: Calculate CDF(a) and PDF(a)\n",
    "    try:\n",
    "        pdf_a = norm.pdf(a)\n",
    "        cdf_a = norm.cdf(a)\n",
    "    except ValueError:\n",
    "        # Catch edge cases where 'a' is extreme\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "    # Step 3: Calculate Virtual Valuation (Myerson for Utility Maximization)\n",
    "    # V(a) = a + CDF(a) / PDF(a) - Selecting the HIGHEST V(a) maximizes utility.\n",
    "    \n",
    "    if pdf_a == 0:\n",
    "        # Assign a high valuation if PDF is zero (extreme 'a' value)\n",
    "        virtual_valuation = float('inf')\n",
    "    else:\n",
    "        # We use V(a) = a + (cdf_a / pdf_a) and select max(V) to maximize utility\n",
    "        virtual_valuation = a + (cdf_a / pdf_a)\n",
    "        \n",
    "    return virtual_valuation\n",
    "\n",
    "def update_ucb_stats(llm_key, utility):\n",
    "    \"\"\"Updates the UCB statistics for the winning LLM arm based on Net Utility (Reward - Cost).\"\"\"\n",
    "    config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Q now tracks total utility (Reward - Cost)\n",
    "    config[\"ucb_Q\"] += utility\n",
    "    config[\"ucb_N\"] += 1\n",
    "    \n",
    "    # mean_reward now tracks mean utility\n",
    "    config[\"ucb_mean_reward\"] = config[\"ucb_Q\"] / config[\"ucb_N\"]\n",
    "\n",
    "\n",
    "# --- 4. EVALUATION LOGIC (Main loop - CORRECTED) ---\n",
    "def calculate_efficiency_with_ucb_myerson(df, query_col):\n",
    "    \"\"\"\n",
    "    Loops through the dataset, gets predictions from both LLMs, \n",
    "    applies UCB/Myerson logic to select the winner, and calculates overall efficiency.\n",
    "    \"\"\"\n",
    "    global GLOBAL_T\n",
    "    \n",
    "    # Data cleaning for ground truth\n",
    "    for col in EVAL_COLUMNS:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "    total_count = len(df)\n",
    "    \n",
    "    # Tracking for final results\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    llm_selection_counts = {key: 0 for key in LLM_CONFIGS.keys()}\n",
    "    total_reward_collected = 0\n",
    "    total_cost_incurred = 0\n",
    "\n",
    "    print(f\"Starting UCB/Reverse-Myerson evaluation on {total_count} countries...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "        GLOBAL_T += 1 # Increment total rounds (t)\n",
    "\n",
    "        # Dictionary to hold the results for both LLMs in this round\n",
    "        round_results = {}\n",
    "        \n",
    "        # 4.1 Get Predictions, Rewards, and Costs for BOTH LLMs\n",
    "        for llm_key in LLM_CONFIGS.keys():\n",
    "            # Run inference for the current LLM\n",
    "            llm_response_dict, raw_output, total_tokens, cost = \\\n",
    "                get_llm_prediction_and_cost(country, llm_key)\n",
    "            \n",
    "            # Calculate Total Reward for this LLM's prediction\n",
    "            current_reward = 0\n",
    "            is_correct_for_llm = {col: False for col in EVAL_COLUMNS}\n",
    "            \n",
    "            for col in EVAL_COLUMNS:\n",
    "                true_value = row[col]\n",
    "                predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "                \n",
    "                is_correct = (predicted_value == true_value)\n",
    "                \n",
    "                # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "                if col in ['Latitude', 'Longitude']:\n",
    "                    true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                    pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                    \n",
    "                    try:\n",
    "                        true_num = float(true_num_str)\n",
    "                        pred_num = float(pred_num_str)\n",
    "                        \n",
    "                        if abs(true_num - pred_num) < 0.05:\n",
    "                            is_correct = True\n",
    "                        else:\n",
    "                            is_correct = False\n",
    "                    except ValueError:\n",
    "                        is_correct = False\n",
    "                \n",
    "                if is_correct:\n",
    "                    current_reward += REWARD_MAP[col]\n",
    "                    is_correct_for_llm[col] = True\n",
    "            \n",
    "            round_results[llm_key] = {\n",
    "                \"reward\": current_reward,\n",
    "                \"cost\": cost,\n",
    "                \"utility\": current_reward - cost, # Store Net Utility\n",
    "                \"is_correct\": is_correct_for_llm,\n",
    "                \"response_dict\": llm_response_dict\n",
    "            }\n",
    "\n",
    "\n",
    "        # 4.2 UCB and Reverse-Myerson Selection (CORRECTED)\n",
    "        \n",
    "        # 1. Calculate Virtual Valuation for each LLM's result\n",
    "        virtual_valuations = {}\n",
    "        for llm_key, result in round_results.items():\n",
    "            virtual_valuations[llm_key] = calculate_virtual_valuation(\n",
    "                llm_key, country, GLOBAL_T, result[\"reward\"], result[\"cost\"]\n",
    "            )\n",
    "\n",
    "        # 2. Select the winner: Highest Virtual Valuation wins (Maximizing Utility)\n",
    "        winning_llm_key = max(virtual_valuations, key=virtual_valuations.get) # CRITICAL FIX: MAX instead of MIN\n",
    "        \n",
    "        winning_result = round_results[winning_llm_key]\n",
    "        \n",
    "        # 4.3 Update Statistics\n",
    "        \n",
    "        # Update UCB/Myerson Arm Stats with the Net Utility (Reward - Cost)\n",
    "        update_ucb_stats(winning_llm_key, winning_result[\"utility\"]) # CRITICAL FIX: Use utility for update\n",
    "        llm_selection_counts[winning_llm_key] += 1\n",
    "        \n",
    "        # Update Overall Evaluation Metrics\n",
    "        total_reward_collected += winning_result[\"reward\"]\n",
    "        total_cost_incurred += winning_result[\"cost\"]\n",
    "        \n",
    "        for col in EVAL_COLUMNS:\n",
    "            if winning_result[\"is_correct\"][col]:\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        if (index + 1) % 10 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries. Winner: {winning_llm_key} | Total Reward: {total_reward_collected:.2f}\")\n",
    "\n",
    "    # 4.4 Final Efficiency Calculation\n",
    "    efficiency = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}%\" \n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "    \n",
    "    total_possible_reward = total_count * sum(REWARD_MAP.values())\n",
    "    \n",
    "    return (efficiency, total_count, correct_counts, llm_selection_counts, \n",
    "            total_reward_collected, total_possible_reward, total_cost_incurred)\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 5.1 Initialize LLMs\n",
    "    initialize_llms()\n",
    "    \n",
    "    # 5.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = calculate_efficiency_with_ucb_myerson(data, QUERY_COLUMN)\n",
    "        (efficiency_results, total, correct, llm_selections, \n",
    "         total_reward, total_possible_reward, total_cost) = results\n",
    "        \n",
    "        # 5.3 Print Final Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ§  Multi-LLM UCB/Reverse-Myerson Evaluation Results (FIXED)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Countries Evaluated (t): {total}\")\n",
    "        print(f\"Total Possible Reward: {total_possible_reward}\")\n",
    "        print(f\"Total Reward Collected: {total_reward:.2f}\")\n",
    "        print(f\"Total Cost Incurred (Mock): ${total_cost:.8f}\")\n",
    "        print(f\"Net Utility (Reward - Cost): {total_reward - total_cost:.2f}\")\n",
    "        \n",
    "        print(\"\\n## Model Selection Counts\")\n",
    "        selection_table = pd.DataFrame([llm_selections]).T\n",
    "        selection_table.columns = ['Times Selected']\n",
    "        selection_table.index.name = 'LLM'\n",
    "        print(selection_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "        \n",
    "        print(\"\\n## Final Accuracy (Based on Winning LLM's Prediction)\")\n",
    "        results_table = pd.DataFrame([efficiency_results]).T\n",
    "        results_table.columns = ['Efficiency']\n",
    "        results_table.index.name = 'Column'\n",
    "        print(results_table.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        print(\"\\nRaw Correct Counts (for the selected winner):\")\n",
    "        for col in EVAL_COLUMNS:\n",
    "            print(f\"- {col}: {correct[col]}/{total} correct\")\n",
    "        \n",
    "        print(\"\\nLLM UCB Statistics (Mean Reward = Mean Utility):\")\n",
    "        for key, config in LLM_CONFIGS.items():\n",
    "             print(f\"- {key}: N={config['ucb_N']}, Mean Utility={config['ucb_mean_reward']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456dcbf-133a-4792-b2a9-36c13ba4f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM pipelines...\n",
      "Loading 1B-Instruct (meta-llama/Llama-3.2-1B-Instruct)...\n",
      "1B-Instruct loaded successfully.\n",
      "Loading 3B-Instruct (meta-llama/Llama-3.2-3B-Instruct)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aacb6742cd47af90901fa165014340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B-Instruct loaded successfully.\n",
      "\n",
      "======================================================================\n",
      "SCENARIO 1: UCB/REVERSE-MYERSON BLENDED APPROACH\n",
      "Starting UCB/Reverse-Myerson evaluation on 196 countries...\n",
      "Processed 10/196 entries. Winner: LLM_3B | Total Reward: 28.00\n",
      "Processed 20/196 entries. Winner: LLM_3B | Total Reward: 54.00\n",
      "Processed 30/196 entries. Winner: LLM_1B | Total Reward: 78.00\n",
      "Processed 40/196 entries. Winner: LLM_3B | Total Reward: 105.00\n",
      "Processed 50/196 entries. Winner: LLM_3B | Total Reward: 135.00\n",
      "Processed 60/196 entries. Winner: LLM_3B | Total Reward: 162.00\n"
     ]
    }
   ],
   "source": [
    "# --- 3.5. BASELINE EVALUATION LOGIC (New Function) ---\n",
    "def evaluate_single_llm(df, llm_key, query_col):\n",
    "    \"\"\"\n",
    "    Evaluates a single LLM across the entire dataset to establish a baseline.\n",
    "    No UCB/Myerson logic is applied.\n",
    "    \"\"\"\n",
    "    llm_config = LLM_CONFIGS[llm_key]\n",
    "    \n",
    "    # Reset UCB stats before running single eval (though not strictly needed here)\n",
    "    llm_config[\"ucb_N\"] = 0\n",
    "    llm_config[\"ucb_Q\"] = 0.0\n",
    "    llm_config[\"ucb_mean_reward\"] = 0.0\n",
    "\n",
    "    total_count = len(df)\n",
    "    correct_counts = {col: 0 for col in EVAL_COLUMNS}\n",
    "    total_reward = 0\n",
    "    total_cost = 0\n",
    "\n",
    "    print(f\"Starting baseline evaluation for {llm_config['name']}...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        country = row[query_col]\n",
    "\n",
    "        # Get Prediction, Reward, and Cost\n",
    "        llm_response_dict, raw_output, total_tokens, cost = \\\n",
    "            get_llm_prediction_and_cost(country, llm_key)\n",
    "        \n",
    "        current_reward = 0\n",
    "        \n",
    "        for col in EVAL_COLUMNS:\n",
    "            true_value = row[col]\n",
    "            predicted_value = str(llm_response_dict.get(col, '')).strip().lower()\n",
    "            is_correct = (predicted_value == true_value)\n",
    "            \n",
    "            # Robust Comparison for Latitude/Longitude (Tolerance 0.05)\n",
    "            if col in ['Latitude', 'Longitude']:\n",
    "                true_num_str = re.sub(r'[^0-9.-]', '', true_value)\n",
    "                pred_num_str = re.sub(r'[^0-9.-]', '', predicted_value)\n",
    "                \n",
    "                try:\n",
    "                    true_num = float(true_num_str)\n",
    "                    pred_num = float(pred_num_str)\n",
    "                    \n",
    "                    if abs(true_num - pred_num) < 0.05:\n",
    "                        is_correct = True\n",
    "                    else:\n",
    "                        is_correct = False\n",
    "                except ValueError:\n",
    "                    is_correct = False\n",
    "            \n",
    "            if is_correct:\n",
    "                current_reward += REWARD_MAP[col]\n",
    "                correct_counts[col] += 1\n",
    "        \n",
    "        total_reward += current_reward\n",
    "        total_cost += cost\n",
    "        \n",
    "        if (index + 1) % 50 == 0 or (index + 1) == total_count:\n",
    "            print(f\"Processed {index + 1}/{total_count} entries for {llm_config['name']}.\")\n",
    "\n",
    "\n",
    "    # Calculate final accuracy percentages\n",
    "    accuracy_results = {\n",
    "        col: f\"{correct_counts[col] / total_count * 100:.2f}\"\n",
    "        for col in EVAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"LLM_Name\": llm_config['name'],\n",
    "        \"Total_Reward\": total_reward,\n",
    "        \"Total_Cost\": total_cost,\n",
    "        \"Net_Utility\": total_reward - total_cost,\n",
    "        \"Correct_Counts\": correct_counts,\n",
    "        \"Accuracy\": accuracy_results\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 5. EXECUTION (MODIFIED TO INCLUDE BASELINES) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 5.1 Initialize LLMs\n",
    "    initialize_llms()\n",
    "    \n",
    "    # 5.2 Main Evaluation Block\n",
    "    try:\n",
    "        # Load the ground truth data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Data cleaning for ground truth (required for single eval too)\n",
    "        for col in EVAL_COLUMNS:\n",
    "            data[col] = data[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "        total_countries = len(data)\n",
    "        \n",
    "        # --- Run all 3 Scenarios ---\n",
    "        \n",
    "        # 1. UCB/Reverse-Myerson Blend (Uses the original, fixed logic)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SCENARIO 1: UCB/REVERSE-MYERSON BLENDED APPROACH\")\n",
    "        results_ucb = calculate_efficiency_with_ucb_myerson(data, QUERY_COLUMN)\n",
    "        \n",
    "        (ucb_efficiency, total, ucb_correct, llm_selections, \n",
    "         ucb_total_reward, total_possible_reward, ucb_total_cost) = results_ucb\n",
    "        \n",
    "        # 2. LLM_1B Baseline\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SCENARIO 2: LLM_1B BASELINE (Solo Run)\")\n",
    "        results_llm1 = evaluate_single_llm(data, 'LLM_1B', QUERY_COLUMN)\n",
    "        \n",
    "        # 3. LLM_3B Baseline\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SCENARIO 3: LLM_3B BASELINE (Solo Run)\")\n",
    "        results_llm3 = evaluate_single_llm(data, 'LLM_3B', QUERY_COLUMN)\n",
    "        \n",
    "        \n",
    "        # --- 5.3 Print Final Results and Comparisons ---\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ† FINAL COMPARATIVE RESULTS: UCB/MYERSON vs. BASELINES\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total Countries Evaluated: {total_countries}\")\n",
    "        \n",
    "        # =================================================================\n",
    "        # TABLE 1: OVERALL PERFORMANCE COMPARISON (Reward, Cost, Utility)\n",
    "        # =================================================================\n",
    "        \n",
    "        print(\"\\n## 1. Overall Performance Comparison (Utility Maximization)\")\n",
    "        \n",
    "        overall_data = {\n",
    "            'Model': [\n",
    "                results_llm1['LLM_Name'] + ' (Baseline)',\n",
    "                results_llm3['LLM_Name'] + ' (Baseline)',\n",
    "                'UCB/Myerson Blend'\n",
    "            ],\n",
    "            'Total Reward': [\n",
    "                results_llm1['Total_Reward'],\n",
    "                results_llm3['Total_Reward'],\n",
    "                ucb_total_reward\n",
    "            ],\n",
    "            'Total Cost (Mock $)': [\n",
    "                f\"${results_llm1['Total_Cost']:.8f}\",\n",
    "                f\"${results_llm3['Total_Cost']:.8f}\",\n",
    "                f\"${ucb_total_cost:.8f}\"\n",
    "            ],\n",
    "            'Net Utility (Reward - Cost)': [\n",
    "                f\"{results_llm1['Net_Utility']:.2f}\",\n",
    "                f\"{results_llm3['Net_Utility']:.2f}\",\n",
    "                f\"{ucb_total_reward - ucb_total_cost:.2f}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        overall_table = pd.DataFrame(overall_data)\n",
    "        print(overall_table.to_markdown(numalign=\"left\", stralign=\"left\", index=False))\n",
    "\n",
    "        # =================================================================\n",
    "        # TABLE 2: ACCURACY COMPARISON (Per Column)\n",
    "        # =================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*45)\n",
    "        print(\"## 2. Accuracy Comparison by Column\")\n",
    "        \n",
    "        accuracy_data = {\n",
    "            'Column': EVAL_COLUMNS,\n",
    "            results_llm1['LLM_Name'] + ' (%)': [results_llm1['Accuracy'][col] for col in EVAL_COLUMNS],\n",
    "            results_llm3['LLM_Name'] + ' (%)': [results_llm3['Accuracy'][col] for col in EVAL_COLUMNS],\n",
    "            'UCB/Myerson Blend (%)': [ucb_efficiency[col].replace('%', '') for col in EVAL_COLUMNS]\n",
    "        }\n",
    "        \n",
    "        accuracy_table = pd.DataFrame(accuracy_data)\n",
    "        print(accuracy_table.to_markdown(numalign=\"left\", stralign=\"left\", index=False))\n",
    "        \n",
    "        # =================================================================\n",
    "        # TABLE 3: UCB/MYERSON SELECTION METRICS\n",
    "        # =================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*55)\n",
    "        print(\"## 3. UCB/Myerson Selection and Learning Metrics\")\n",
    "        \n",
    "        selection_data = {\n",
    "            'LLM': list(LLM_CONFIGS.keys()),\n",
    "            'Times Selected (N)': [LLM_CONFIGS[key]['ucb_N'] for key in LLM_CONFIGS.keys()],\n",
    "            'Total Utility (Q)': [f\"{LLM_CONFIGS[key]['ucb_Q']:.4f}\" for key in LLM_CONFIGS.keys()],\n",
    "            'Mean Utility (UÌ„)': [f\"{LLM_CONFIGS[key]['ucb_mean_reward']:.4f}\" for key in LLM_CONFIGS.keys()]\n",
    "        }\n",
    "        \n",
    "        selection_table = pd.DataFrame(selection_data)\n",
    "        print(selection_table.to_markdown(numalign=\"left\", stralign=\"left\", index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFATAL ERROR: The file was not found at the configured path:\\n{file_path}\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unhandled error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f12073-5f12-4023-86ab-49deb60b6e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Llama Env)",
   "language": "python",
   "name": "my_llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
